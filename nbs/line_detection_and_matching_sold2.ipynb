{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/line_detection_and_matching_sold2.ipynb)\n",
    "\n",
    "# Line detection and matching example with SOLD2: Self-supervised Occlusion-aware Line Description and Detection\n",
    "\n",
    "In this tutorial we will show how we can quickly perform line detection, and matching using [`kornia.feature.sold2`](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.SOLD2_detector) API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Install the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/kornia/kornia\n",
    "!pip install opencv-python --upgrade # Just for windows\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/cvg/SOLD2/raw/main/assets/images/terrace0.JPG\n",
    "!wget https://github.com/cvg/SOLD2/raw/main/assets/images/terrace1.JPG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we will load the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the images and convert into torch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(img_path):\n",
    "    try:\n",
    "        # not ready on Windows machine\n",
    "        img = K.io.load_image(img_path, K.io.ImageLoadType.RGB32)\n",
    "    except:\n",
    "        import cv2\n",
    "\n",
    "        img = cv2.imread(img_path, cv2.IMREAD_COLOR)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = K.image_to_tensor(img).float() / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname1 = \"terrace0.JPG\"\n",
    "fname2 = \"terrace1.JPG\"\n",
    "\n",
    "torch_img1 = load_img(fname1)\n",
    "torch_img2 = load_img(fname2)\n",
    "\n",
    "torch_img1.shape, torch_img2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the data for the model, which is expected a batch of images in gray scale (shape: (Batch size, 1, Height, Width)).\n",
    "\n",
    "The SOLD2 model was tuned for images in the range 400~800px when using `config=None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, convert the images to gray scale\n",
    "torch_img1_gray = K.color.rgb_to_grayscale(torch_img1)\n",
    "torch_img2_gray = K.color.rgb_to_grayscale(torch_img2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_img1_gray.shape, torch_img2_gray.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# then, stack the images to create/simulate a batch\n",
    "imgs = torch.stack(\n",
    "    [torch_img1_gray, torch_img2_gray],\n",
    ")\n",
    "\n",
    "imgs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performs line detection and matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the sold2 model with `pre-trained=True`, which will download and set pre-trained weights to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "sold2 = KF.SOLD2(pretrained=True, config=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "with torch.inference_mode():\n",
    "    outputs = sold2(imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize the outputs for demo.\n",
    "\n",
    "**Attention:** The detected line segments is in *ij coordinates convention*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line_seg1 = outputs[\"line_segments\"][0]\n",
    "line_seg2 = outputs[\"line_segments\"][1]\n",
    "desc1 = outputs[\"dense_desc\"][0]\n",
    "desc2 = outputs[\"dense_desc\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform line matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    matches = sold2.match(line_seg1, line_seg2, desc1[None], desc2[None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_matches = matches != -1\n",
    "match_indices = matches[valid_matches]\n",
    "\n",
    "matched_lines1 = line_seg1[valid_matches]\n",
    "matched_lines2 = line_seg2[match_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot lines detected and also the match\n",
    "\n",
    "Plot functions adapted from [original code](https://github.com/cvg/SOLD2/blob/ddd36788c112136be2975ee29b096df979571bb2/sold2/misc/visualize_util.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def plot_images(imgs, titles=None, cmaps=\"gray\", dpi=100, size=6, pad=0.5):\n",
    "    \"\"\"Plot a set of images horizontally.\n",
    "    Args:\n",
    "        imgs: a list of NumPy or PyTorch images, RGB (H, W, 3) or mono (H, W).\n",
    "        titles: a list of strings, as titles for each image.\n",
    "        cmaps: colormaps for monochrome images.\n",
    "    \"\"\"\n",
    "    n = len(imgs)\n",
    "    if not isinstance(cmaps, (list, tuple)):\n",
    "        cmaps = [cmaps] * n\n",
    "    figsize = (size * n, size * 3 / 4) if size is not None else None\n",
    "    fig, ax = plt.subplots(1, n, figsize=figsize, dpi=dpi)\n",
    "    if n == 1:\n",
    "        ax = [ax]\n",
    "    for i in range(n):\n",
    "        ax[i].imshow(imgs[i], cmap=plt.get_cmap(cmaps[i]))\n",
    "        ax[i].get_yaxis().set_ticks([])\n",
    "        ax[i].get_xaxis().set_ticks([])\n",
    "        ax[i].set_axis_off()\n",
    "        for spine in ax[i].spines.values():  # remove frame\n",
    "            spine.set_visible(False)\n",
    "        if titles:\n",
    "            ax[i].set_title(titles[i])\n",
    "    fig.tight_layout(pad=pad)\n",
    "\n",
    "\n",
    "def plot_lines(lines, line_colors=\"orange\", point_colors=\"cyan\", ps=4, lw=2, indices=(0, 1)):\n",
    "    \"\"\"Plot lines and endpoints for existing images.\n",
    "    Args:\n",
    "        lines: list of ndarrays of size (N, 2, 2).\n",
    "        colors: string, or list of list of tuples (one for each keypoints).\n",
    "        ps: size of the keypoints as float pixels.\n",
    "        lw: line width as float pixels.\n",
    "        indices: indices of the images to draw the matches on.\n",
    "    \"\"\"\n",
    "    if not isinstance(line_colors, list):\n",
    "        line_colors = [line_colors] * len(lines)\n",
    "    if not isinstance(point_colors, list):\n",
    "        point_colors = [point_colors] * len(lines)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    ax = fig.axes\n",
    "    assert len(ax) > max(indices)\n",
    "    axes = [ax[i] for i in indices]\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Plot the lines and junctions\n",
    "    for a, l, lc, pc in zip(axes, lines, line_colors, point_colors):\n",
    "        for i in range(len(l)):\n",
    "            line = matplotlib.lines.Line2D(\n",
    "                (l[i, 1, 1], l[i, 0, 1]),\n",
    "                (l[i, 1, 0], l[i, 0, 0]),\n",
    "                zorder=1,\n",
    "                c=lc,\n",
    "                linewidth=lw,\n",
    "            )\n",
    "            a.add_line(line)\n",
    "        pts = l.reshape(-1, 2)\n",
    "        a.scatter(pts[:, 1], pts[:, 0], c=pc, s=ps, linewidths=0, zorder=2)\n",
    "\n",
    "\n",
    "def plot_color_line_matches(lines, lw=2, indices=(0, 1)):\n",
    "    \"\"\"Plot line matches for existing images with multiple colors.\n",
    "    Args:\n",
    "        lines: list of ndarrays of size (N, 2, 2).\n",
    "        lw: line width as float pixels.\n",
    "        indices: indices of the images to draw the matches on.\n",
    "    \"\"\"\n",
    "    n_lines = len(lines[0])\n",
    "\n",
    "    cmap = plt.get_cmap(\"nipy_spectral\", lut=n_lines)\n",
    "    colors = np.array([mcolors.rgb2hex(cmap(i)) for i in range(cmap.N)])\n",
    "\n",
    "    np.random.shuffle(colors)\n",
    "\n",
    "    fig = plt.gcf()\n",
    "    ax = fig.axes\n",
    "    assert len(ax) > max(indices)\n",
    "    axes = [ax[i] for i in indices]\n",
    "    fig.canvas.draw()\n",
    "\n",
    "    # Plot the lines\n",
    "    for a, l in zip(axes, lines):\n",
    "        for i in range(len(l)):\n",
    "            line = matplotlib.lines.Line2D(\n",
    "                (l[i, 1, 1], l[i, 0, 1]),\n",
    "                (l[i, 1, 0], l[i, 0, 0]),\n",
    "                zorder=1,\n",
    "                c=colors[i],\n",
    "                linewidth=lw,\n",
    "            )\n",
    "            a.add_line(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs_to_plot = [K.tensor_to_image(torch_img1), K.tensor_to_image(torch_img2)]\n",
    "lines_to_plot = [line_seg1.numpy(), line_seg2.numpy()]\n",
    "\n",
    "plot_images(imgs_to_plot, [\"Image 1 - detected lines\", \"Image 2 - detected lines\"])\n",
    "plot_lines(lines_to_plot, ps=3, lw=2, indices={0, 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(imgs_to_plot, [\"Image 1 - matched lines\", \"Image 2 - matched lines\"])\n",
    "plot_color_line_matches([matched_lines1, matched_lines2], lw=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of homography from line segment correspondences from SOLD2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Robust geometry estimation with Random sample consensus (RANSAC)\n",
    "\n",
    "Load the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ransac = K.geometry.RANSAC(model_type=\"homography_from_linesegments\", inl_th=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform the model correspondencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_ransac, correspondence_mask = ransac(matched_lines1.flip(dims=(2,)), matched_lines2.flip(dims=(2,)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap the image 1 to image 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img1_warp_to2 = K.geometry.warp_perspective(torch_img1[None], H_ransac[None], (torch_img1.shape[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the matched lines and wrapped image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(\n",
    "    imgs_to_plot,\n",
    "    [\"Image 1 - lines with correspondence\", \"Image 2 - lines with correspondence\"],\n",
    ")\n",
    "plot_color_line_matches([matched_lines1[correspondence_mask], matched_lines2[correspondence_mask]], lw=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(\n",
    "    [K.tensor_to_image(torch_img2), K.tensor_to_image(img1_warp_to2)],\n",
    "    [\"Image 2\", \"Image 1 wrapped to 2\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
