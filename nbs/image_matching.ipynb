{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/image_matching.ipynb)\n",
    "\n",
    "# Image matching example with LoFTR\n",
    "\n",
    "First, we will install everything needed:\n",
    "\n",
    "\n",
    "\n",
    "*  fresh version of [kornia](https://github.com/kornia/kornia) for [LoFTR](https://zju3dv.github.io/loftr/)\n",
    "*  fresh version of OpenCV for MAGSAC++ geometry estimation\n",
    "*  [kornia_moons](https://ducha-aiki.github.io/kornia_moons) for the conversions and visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install kornia\n",
    "!pip install kornia_moons\n",
    "!pip install opencv-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download an image pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\n",
    "!wget https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define image matching pipeline with OpenCV SIFT features. We will also use kornia for the state-of-the-art match filtering -- Lowe ratio + mutual nearest neighbor check and [MAGSAC++](https://ducha-aiki.github.io/wide-baseline-stereo-blog/2021/05/17/OpenCV-New-RANSACs.html) as RANSAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "\n",
    "def load_torch_image(fname):\n",
    "    img = K.image_to_tensor(cv2.imread(fname), False).float() / 255.0\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fname1 = \"kn_church-2.jpg\"\n",
    "fname2 = \"kn_church-8.jpg\"\n",
    "\n",
    "img1 = load_torch_image(fname1)\n",
    "img2 = load_torch_image(fname2)\n",
    "\n",
    "\n",
    "matcher = KF.LoFTR(pretrained=\"outdoor\")\n",
    "\n",
    "input_dict = {\n",
    "    \"image0\": K.color.rgb_to_grayscale(img1),  # LofTR works on grayscale images only\n",
    "    \"image1\": K.color.rgb_to_grayscale(img2),\n",
    "}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    correspondences = matcher(input_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in correspondences.items():\n",
    "    print(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's clean-up the correspondences with modern RANSAC and estimate fundamental matrix between two images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkpts0 = correspondences[\"keypoints0\"].cpu().numpy()\n",
    "mkpts1 = correspondences[\"keypoints1\"].cpu().numpy()\n",
    "Fm, inliers = cv2.findFundamentalMat(mkpts0, mkpts1, cv2.USAC_MAGSAC, 0.5, 0.999, 100000)\n",
    "inliers = inliers > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's draw the matches with a function from [kornia_moons](https://ducha-aiki.github.io/kornia_moons/feature.html#draw_LAF_matches). The correct matches are in green and imprecise matches - in blue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_LAF_matches(\n",
    "    KF.laf_from_center_scale_ori(\n",
    "        torch.from_numpy(mkpts0).view(1, -1, 2),\n",
    "        torch.ones(mkpts0.shape[0]).view(1, -1, 1, 1),\n",
    "        torch.ones(mkpts0.shape[0]).view(1, -1, 1),\n",
    "    ),\n",
    "    KF.laf_from_center_scale_ori(\n",
    "        torch.from_numpy(mkpts1).view(1, -1, 2),\n",
    "        torch.ones(mkpts1.shape[0]).view(1, -1, 1, 1),\n",
    "        torch.ones(mkpts1.shape[0]).view(1, -1, 1),\n",
    "    ),\n",
    "    torch.arange(mkpts0.shape[0]).view(-1, 1).repeat(1, 2),\n",
    "    K.tensor_to_image(img1),\n",
    "    K.tensor_to_image(img2),\n",
    "    inliers,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": (0.2, 0.5, 1), \"vertical\": False},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
