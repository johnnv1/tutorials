{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/image_registration.ipynb)\n",
    "\n",
    "# Image Registration by Direct Optimization\n",
    "\n",
    "In this tutorial we are going to learn how to perform the task of image alignment by optimising the similarity transformation between two images in order to create a photo with wide in-focus area from set of narrow-focused images.\n",
    "The images are courtesy of [Dennis Sakva](https://twitter.com/DennisSakva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/kornia/kornia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget http://cmp.felk.cvut.cz/~mishkdmy/bee.zip\n",
    "!unzip bee.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import needed libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from copy import deepcopy\n",
    "from typing import List\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import kornia as K\n",
    "import kornia.geometry as KG\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def get_data_directory(base):\n",
    "    path = os.path.join(\"../\", base)\n",
    "    if os.path.isdir(os.path.join(path, \"data\")):\n",
    "        return os.path.join(path, \"data/\")\n",
    "    return get_data_directory(path)\n",
    "\n",
    "\n",
    "def load_timg(file_name):\n",
    "    \"\"\"Loads the image with OpenCV and converts to torch.Tensor\"\"\"\n",
    "    assert os.path.isfile(file_name), f\"Invalid file {file_name}\"\n",
    "    # load image with OpenCV\n",
    "    img = cv2.imread(file_name, cv2.IMREAD_COLOR)\n",
    "    # convert image to torch tensor\n",
    "    tensor = K.image_to_tensor(img, None).float() / 255.0\n",
    "    return K.color.bgr_to_rgb(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Images preview\n",
    "\n",
    "Let's check our images. There are almost 100 of them, so we will show only each 10th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = os.listdir(\"bee\")\n",
    "fnames = [f\"bee/{x}\" for x in sorted(fnames) if x.endswith(\"JPG\")]\n",
    "fig, axis = plt.subplots(2, 5, figsize=(12, 4), sharex=\"all\", sharey=\"all\", frameon=False)\n",
    "for i, fname in enumerate(fnames):\n",
    "    if i % 10 != 0:\n",
    "        continue\n",
    "    j = i // 10\n",
    "    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n",
    "    axis[j // 5][j % 5].imshow(img, aspect=\"auto\")\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the focus goes from back to the front, so we have to match and merge them in the same order.\n",
    "\n",
    "## Image registration\n",
    "\n",
    "We will need `ImageRegistrator` object to do the matching. Because the photos are takes so that only slight rotation, shift and scale change is possible, we will use `similarity` mode, which does exactly this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda: bool = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "registrator = KG.ImageRegistrator(\"similarity\", loss_fn=F.mse_loss, lr=8e-4, pyramid_levels=3, num_iterations=500).to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will register images sequentially with `ImageRegistrator`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "models = []\n",
    "for i, fname in tqdm(enumerate(fnames)):\n",
    "    if i == 0:\n",
    "        continue\n",
    "    prev_img = load_timg(fnames[i - 1]).to(device)\n",
    "    curr_img = load_timg(fname).to(device)\n",
    "    model = registrator.register(prev_img, curr_img)\n",
    "    models.append(deepcopy(model.detach()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the final (the most close-focused) image as the reference - this means that we have to convert our image transforms from (between `i` and `i+1`) mode into (between `i` and last). We can do it by matrix multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_final = [torch.eye(3, device=device)[None]]\n",
    "for m in models[::-1]:\n",
    "    models_to_final.append(m @ models_to_final[-1])\n",
    "models_to_final = models_to_final[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what do we got. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = plt.subplots(2, 5, figsize=(12, 4), sharex=\"all\", sharey=\"all\", frameon=False)\n",
    "for i, fname in enumerate(fnames):\n",
    "    if i % 10 != 0:\n",
    "        continue\n",
    "    timg = load_timg(fname).to(device)\n",
    "    j = i // 10\n",
    "    timg_dst = KG.homography_warp(timg, models_to_final[i], timg.shape[-2:])\n",
    "    axis[j // 5][j % 5].imshow(K.tensor_to_image(timg_dst * 255.0).astype(np.uint8), aspect=\"auto\")\n",
    "plt.subplots_adjust(wspace=0.05, hspace=0.05)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will merge the image sequence into single image. \n",
    "The idea is to detect the image parts, which are in focus from the current image and blend them into the final images.\n",
    "To get the sharp image part we can use `kornia.filters.laplacian`. \n",
    "Then we reproject image1 into image2, and merge them using mask we created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_sharp1_into2(timg1, timg2, trans1to2, verbose=False):\n",
    "    curr_img = timg2.clone()\n",
    "    warped = KG.homography_warp(timg1, torch.inverse(trans1to2), timg.shape[-2:])\n",
    "    mask1 = K.filters.laplacian(K.color.rgb_to_grayscale(timg1), 7).abs()\n",
    "    mask1_norm = (mask1 - mask1.min()) / (mask1.max() - mask1.min())\n",
    "    mask1_blur = K.filters.gaussian_blur2d(mask1_norm, (9, 9), (1.6, 1.6))\n",
    "    mask1_blur = mask1_blur / mask1_blur.max()\n",
    "    warped_mask = KG.homography_warp(mask1_blur.float(), torch.inverse(models_to_final[i]), timg1.shape[-2:])\n",
    "    curr_img = warped_mask * warped + (1 - warped_mask) * curr_img\n",
    "    if verbose:\n",
    "        fig, axis = plt.subplots(1, 4, figsize=(15, 6), sharex=\"all\", sharey=\"all\", frameon=False)\n",
    "        axis[0].imshow(K.tensor_to_image(timg1))\n",
    "        axis[1].imshow(K.tensor_to_image(mask1_blur))\n",
    "        axis[2].imshow(K.tensor_to_image(timg2))\n",
    "        axis[3].imshow(K.tensor_to_image(curr_img))\n",
    "        axis[0].set_title(\"Img1\")\n",
    "        axis[1].set_title(\"Sharp mask on img1\")\n",
    "        axis[2].set_title(\"Img2\")\n",
    "        axis[3].set_title(\"Blended image\")\n",
    "    return curr_img\n",
    "\n",
    "\n",
    "timg1 = load_timg(fnames[50]).to(device)\n",
    "timg2 = load_timg(fnames[-1]).to(device)\n",
    "out = merge_sharp1_into2(timg1, timg2, models_to_final[50], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blending does not look really good, but that is because we are trying to merge non-consequtive images with very different focus. Let's try to apply it sequentially and see, what happens.\n",
    "\n",
    "We will also create a video of our sharpening process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "base_img = load_timg(fnames[-1])\n",
    "curr_img = deepcopy(base_img)\n",
    "try:\n",
    "    video_writer = imageio.get_writer(\"sharpening.avi\", fps=8)\n",
    "    video_writer.append_data((K.tensor_to_image(curr_img) * 255.0).astype(np.uint8))\n",
    "    video_ok = True\n",
    "except:\n",
    "    video_ok = False\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, fname in tqdm(enumerate(fnames)):\n",
    "        timg = load_timg(fname)\n",
    "        curr_img = merge_sharp1_into2(timg.to(device), curr_img.to(device), models_to_final[i].to(device))\n",
    "        if video_ok:\n",
    "            video_writer.append_data((K.tensor_to_image(curr_img) * 255.0).astype(np.uint8))\n",
    "if video_ok:\n",
    "    video_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(K.tensor_to_image(curr_img.float()))\n",
    "plt.title(\"Final result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can play the video of our sharpening. The code is ugly to allow running from Google Colab (as shown [here](https://code.luasoftware.com/tutorials/jupyter/google-colab-play-video/))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from base64 import b64encode\n",
    "\n",
    "from IPython.display import HTML\n",
    "\n",
    "if video_ok:\n",
    "    mp4 = open(\"sharpening.avi\", \"rb\").read()\n",
    "else:\n",
    "    mp4 = open(get_data_directory(\"\") + \"sharpening.mp4\", \"rb\").read()\n",
    "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
    "\n",
    "\n",
    "HTML(\n",
    "    f\"\"\"\n",
    "<video width=400 controls>\n",
    "      <source src=\"{data_url}\" type=\"video/mp4\">\n",
    "</video>\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result looks quite nice and more detailed, although a bit soft. You can try yourself different blending parameters yourself (e.g. blur kernel size) in order to improve the final result."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
