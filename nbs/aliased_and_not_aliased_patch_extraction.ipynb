{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/aliased-and-not-aliased-patch-extraction.ipynb)\n",
    "\n",
    "# Image anti-alias with local features\n",
    "\n",
    "In this example we will show the benefits of using anti-aliased patch extraction with kornia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install kornia seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/kornia/data/raw/main/drslump.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, lets load some image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import seaborn as sns\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_original = cv2.cvtColor(cv2.imread(\"drslump.jpg\"), cv2.COLOR_BGR2RGB)\n",
    "plt.figure()\n",
    "plt.imshow(img_original)\n",
    "H, W, CH = img_original.shape\n",
    "\n",
    "DOWNSAMPLE = 4\n",
    "img_small = cv2.resize(img_original, (W // DOWNSAMPLE, H // DOWNSAMPLE), interpolation=cv2.INTER_AREA)\n",
    "plt.figure()\n",
    "plt.imshow(img_small)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets define a keypoint with a large support region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lafs(img, lafs, idx=0, color=\"r\", figsize=(10, 7)):\n",
    "    x, y = KF.laf.get_laf_pts_to_draw(lafs, idx)\n",
    "    plt.figure(figsize=figsize)\n",
    "    if type(img) is torch.tensor:\n",
    "        img_show = K.tensor_to_image(img)\n",
    "    else:\n",
    "        img_show = img\n",
    "    plt.imshow(img_show)\n",
    "    plt.plot(x, y, color)\n",
    "    return\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "laf_orig = torch.tensor([[150.0, 0, 180], [0, 150, 280]]).float().view(1, 1, 2, 3)\n",
    "laf_small = laf_orig / float(DOWNSAMPLE)\n",
    "\n",
    "show_lafs(img_original, laf_orig, figsize=(6, 4))\n",
    "show_lafs(img_small, laf_small, figsize=(6, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets compare how extracted patch would look like when extracted in a naive way and from scale pyramid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS = 32\n",
    "with torch.no_grad():\n",
    "    timg_original = K.image_to_tensor(img_original, False).float().to(device) / 255.0\n",
    "    patches_pyr_orig = KF.extract_patches_from_pyramid(timg_original, laf_orig.to(device), PS)\n",
    "    patches_simple_orig = KF.extract_patches_simple(timg_original, laf_orig.to(device), PS)\n",
    "\n",
    "    timg_small = K.image_to_tensor(img_small, False).float().to(device) / 255.0\n",
    "    patches_pyr_small = KF.extract_patches_from_pyramid(timg_small, laf_small.to(device), PS)\n",
    "    patches_simple_small = KF.extract_patches_simple(timg_small, laf_small.to(device), PS)\n",
    "\n",
    "# Now we will glue all the patches together:\n",
    "\n",
    "\n",
    "def vert_cat_with_margin(p1, p2, margin=3):\n",
    "    b, n, ch, h, w = p1.size()\n",
    "    return torch.cat([p1, torch.ones(b, n, ch, h, margin).to(device), p2], dim=4)\n",
    "\n",
    "\n",
    "def horiz_cat_with_margin(p1, p2, margin=3):\n",
    "    b, n, ch, h, w = p1.size()\n",
    "    return torch.cat([p1, torch.ones(b, n, ch, margin, w).to(device), p2], dim=3)\n",
    "\n",
    "\n",
    "patches_pyr = vert_cat_with_margin(patches_pyr_orig, patches_pyr_small)\n",
    "patches_naive = vert_cat_with_margin(patches_simple_orig, patches_simple_small)\n",
    "\n",
    "patches_all = horiz_cat_with_margin(patches_naive, patches_pyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets show the result. \n",
    "Top row is what you get if you are extracting patches without any antialiasing - note how the patches extracted from the images of different sizes differ.\n",
    "\n",
    "Bottom row is patches, which are extracted from images of different sizes using a scale pyramid. They are not yet exactly the same, but the difference is much smaller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(K.tensor_to_image(patches_all[0, 0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check how much it influences local descriptor performance such as HardNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hardnet = KF.HardNet(True).eval()\n",
    "all_patches = (\n",
    "    torch.cat([patches_pyr_orig, patches_pyr_small, patches_simple_orig, patches_simple_small], dim=0)\n",
    "    .squeeze(1)\n",
    "    .mean(dim=1, keepdim=True)\n",
    ")\n",
    "with torch.no_grad():\n",
    "    descs = hardnet(all_patches)\n",
    "    distances = torch.cdist(descs, descs)\n",
    "    print(distances.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the descriptor difference between antialiased patches is 0.09 and between naively extracted -- 0.44"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
