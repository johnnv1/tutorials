{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.sandbox.google.com/github/kornia/tutorials/blob/master/source/image_matching_adalam.ipynb)\n",
    "\n",
    "# Image matching example with KeyNet-AdaLAM\n",
    "\n",
    "First, we will install everything needed:\n",
    "\n",
    "\n",
    "*  fresh version of [kornia](https://github.com/kornia/kornia) for [AdaLAM](https://arxiv.org/abs/2006.04250)\n",
    "*  fresh version of OpenCV for MAGSAC++ geometry estimation\n",
    "*  [kornia_moons](https://ducha-aiki.github.io/kornia_moons) for the conversions and visualization\n",
    "\n",
    "Docs: [`match_adalam`](https://kornia.readthedocs.io/en/latest/feature.html#kornia.feature.match_adalam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install git+https://github.com/kornia/kornia\n",
    "!pip install kornia_moons\n",
    "!pip install opencv-python --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's download an image pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!wget https://github.com/kornia/data/raw/main/matching/kn_church-2.jpg\n",
    "!wget https://github.com/kornia/data/raw/main/matching/kn_church-8.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from kornia_moons.feature import *\n",
    "\n",
    "\n",
    "def load_torch_image(fname):\n",
    "    img = K.image_to_tensor(cv2.imread(fname), False).float() / 255.0\n",
    "    img = K.color.bgr_to_rgb(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "# device = K.utils.get_cuda_device_if_available()\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "fname1 = \"kn_church-2.jpg\"\n",
    "fname2 = \"kn_church-8.jpg\"\n",
    "\n",
    "img1 = load_torch_image(fname1)\n",
    "img2 = load_torch_image(fname2)\n",
    "\n",
    "\n",
    "feature = KF.KeyNetAffNetHardNet(5000, True).eval().to(device)\n",
    "\n",
    "input_dict = {\n",
    "    \"image0\": K.color.rgb_to_grayscale(img1),  # LofTR works on grayscale images only\n",
    "    \"image1\": K.color.rgb_to_grayscale(img2),\n",
    "}\n",
    "\n",
    "hw1 = torch.tensor(img1.shape[2:])\n",
    "hw2 = torch.tensor(img1.shape[2:])\n",
    "\n",
    "adalam_config = {\"device\": device}\n",
    "\n",
    "with torch.inference_mode():\n",
    "    lafs1, resps1, descs1 = feature(K.color.rgb_to_grayscale(img1))\n",
    "    lafs2, resps2, descs2 = feature(K.color.rgb_to_grayscale(img2))\n",
    "    dists, idxs = KF.match_adalam(\n",
    "        descs1.squeeze(0),\n",
    "        descs2.squeeze(0),\n",
    "        lafs1,\n",
    "        lafs2,  # Adalam takes into account also geometric information\n",
    "        config=adalam_config,\n",
    "        hw1=hw1,\n",
    "        hw2=hw2,  # Adalam also benefits from knowing image size\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{idxs.shape[0]} tentative matches with AdaLAM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_matching_keypoints(lafs1, lafs2, idxs):\n",
    "    mkpts1 = KF.get_laf_center(lafs1).squeeze()[idxs[:, 0]].detach().cpu().numpy()\n",
    "    mkpts2 = KF.get_laf_center(lafs2).squeeze()[idxs[:, 1]].detach().cpu().numpy()\n",
    "    return mkpts1, mkpts2\n",
    "\n",
    "\n",
    "mkpts1, mkpts2 = get_matching_keypoints(lafs1, lafs2, idxs)\n",
    "\n",
    "Fm, inliers = cv2.findFundamentalMat(mkpts1, mkpts2, cv2.USAC_MAGSAC, 0.75, 0.999, 100000)\n",
    "inliers = inliers > 0\n",
    "print(f\"{inliers.sum()} inliers with AdaLAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw the inliers in green and tentative correspondences in yellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_LAF_matches(\n",
    "    lafs1,\n",
    "    lafs2,\n",
    "    idxs,\n",
    "    K.tensor_to_image(img1),\n",
    "    K.tensor_to_image(img2),\n",
    "    inliers,\n",
    "    draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
